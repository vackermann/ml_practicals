{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-87dbb5ed8306>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpylab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'figure.figsize'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "from matplotlib import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisations\n",
    "\n",
    "Let us write some helper functions to initialise weights and biases. We'll initialise weights as Gaussian random variables with mean 0 and variance 0.0025. For biases we'll initialise everything with a constant 0.1. This is because we're mainly going to be using ReLU non-linearities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.05)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Let's define the model. The model is defined as follows:\n",
    "\n",
    "* An input that is 728 dimensional vector. \n",
    "* Reshape the input as 28x28x1 images (only 1 because they are grey scale) \n",
    "* A convolutional layer with 25 filters of shape 12x12x1 and a ReLU non-linearity (with stride (2, 2) and no padding)\n",
    "* A convolutional layer with 64 filters of shape 5x5x25 and a ReLU non-linearity (with stride (1, 2) and padding to maintain size)\n",
    "* A max_pooling layer of shape 2x2\n",
    "* A fully connected layer taking all the outputs of the max_pooling layer to 1024 units and ReLU nonlinearity\n",
    "* A fully connected layer taking 1024 units to 10 no activation function (the softmax non-linearity will be included in the loss function rather than in the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "x_ = tf.reshape(x, [-1, 28, 28, 1])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "\n",
    "# Define the first convolution layer here\n",
    "\n",
    "# Layer configuration\n",
    "filter_size_conv1 = 12\n",
    "num_input_channels_conv1 = 1\n",
    "num_filters_conv1 = 25\n",
    "strides_conv1 = (2,2)\n",
    "# The padding is set to 'VALID' which means the input image is not padded.\n",
    "padding_conv1 = 'VALID'\n",
    "name_conv1 ='CONV1'\n",
    "\n",
    "# Weights shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "W_shape_conv1 = [filter_size_conv1, filter_size_conv1, num_input_channels_conv1, num_filters_conv1]\n",
    "\n",
    "# Create new weights aka. filters with the given shape.\n",
    "W_conv1 = weight_variable(W_shape_conv1)\n",
    "\n",
    "# Bias shape = [num_filters]\n",
    "b_shape_conv1 = [num_filters_conv1]\n",
    "\n",
    "# Create new biases, one for each filter.\n",
    "b_conv1 = bias_variable(b_shape_conv1)\n",
    "\n",
    "# Create convolutional layer\n",
    "h_conv1 = tf.layers.conv2d(inputs=x_, \n",
    "                           filters = num_filters_conv1, # or = W_conv1?\n",
    "                           kernel_size = (filter_size_conv1, filter_size_conv1),\n",
    "                           strides = strides_conv1,\n",
    "                           padding = padding_conv1,\n",
    "                           name = name_conv1,\n",
    "                           activation = tf.nn.relu)\n",
    "#with tf.variable_scope(name_conv1,reuse=True):\n",
    "#   W_conv1 = tf.get_variable(\"kernel\")\n",
    "\n",
    "# Define the second convolution layer here\n",
    "\n",
    "# Layer configuration\n",
    "filter_size_conv2 = 5\n",
    "num_input_channels_conv2 = 25\n",
    "num_filters_conv2 = 54\n",
    "strides_conv2 = (1,2)\n",
    "# The padding is set to 'SAME' which means the input image\n",
    "# is padded with zeroes so the size of the output is the same.\n",
    "padding_conv2 = 'SAME'\n",
    "name_conv2 = 'CONV2'\n",
    "\n",
    "# Weights shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "W_shape_conv2 = [filter_size_conv2, filter_size_conv2, num_input_channels_conv2, num_filters_conv2]\n",
    "\n",
    "# Create new weights aka. filters with the given shape.\n",
    "W_conv2 = weight_variable(W_shape_conv2)\n",
    "\n",
    "# Bias shape = [num_filters]\n",
    "b_shape_conv2 = [num_filters_conv2]\n",
    "\n",
    "# Create new biases, one for each filter.\n",
    "b_conv2 = bias_variable(b_shape_conv2)\n",
    "\n",
    "# Create convolutional layer\n",
    "h_conv2 = tf.layers.conv2d(inputs=h_conv1,\n",
    "                           filters = num_filters_conv2,\n",
    "                           kernel_size = (filter_size_conv2, filter_size_conv2),\n",
    "                           strides = strides_conv2,\n",
    "                           padding = padding_conv2,\n",
    "                           name = name_conv2,\n",
    "                           activation = tf.nn.relu)\n",
    "\n",
    "# Define maxpooling\n",
    "\n",
    "pool_size_pool2 = (2,2)\n",
    "strides_pool2 = 2\n",
    "name_pool2 = 'POOL'\n",
    "h_pool2 = tf.layers.max_pooling2d(inputs=h_conv2,\n",
    "                                  pool_size = pool_size_pool2,\n",
    "                                  strides = strides_pool2,\n",
    "                                  name= name_pool2)\n",
    "\n",
    "# All subsequent layers will be fully connected ignoring geometry so we'll flatten the layer\n",
    "# Flatten the h_pool2_layer (as it has a multidimensiona shape) \n",
    "name_pool2_flat = 'POOL_FLAT'\n",
    "\n",
    "h_pool2_flat = tf.layers.flatten(inputs=h_pool2,\n",
    "                                name=name_pool2_flat)\n",
    "\n",
    "\n",
    "# Define the first fully connected layer here\n",
    "# W_fc1 = \n",
    "# b_fc1 = \n",
    "units_fc1 = 1024\n",
    "name_fc1 = \"FC1\"\n",
    "h_fc1 = tf.layers.dense(inputs=h_pool2_flat,\n",
    "                        units=units_fc1,\n",
    "                        name = name_fc1,\n",
    "                        activation=tf.nn.relu)\n",
    "\n",
    "# Use dropout for this layer (should you wish)\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# The final fully connected layer\n",
    "# W_fc2 = \n",
    "# b_fc2 = \n",
    "units_conv = 10\n",
    "name_conv = \"FC_FINAL\"\n",
    "y_conv = tf.layers.dense(inputs=h_fc1,\n",
    "                        name = name_conv,\n",
    "                        units=units_conv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function, Accuracy and Training Algorithm\n",
    "\n",
    "* We'll use the cross entropy loss function. The loss function is called `tf.nn.cross_entropy_with_logits` in tensorflow\n",
    "\n",
    "* Accuray is simply defined as the fraction of data correctly classified\n",
    "\n",
    "* For training you should use the AdamOptimizer (read the documentation) and set the learning rate to be 1e-4. You are welcome, and in fact encouraged, to experiment with other optimisation procedures and learning rates. \n",
    "\n",
    "* (Optional): You may even want to use different filter sizes once you are finished with experimenting with what is asked in this practial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the cross entropy loss function \n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_conv, labels=y_))\n",
    "\n",
    "# And classification accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# And the Adam optimiser\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mnist data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let us visualise the first 16 data points from the MNIST training data\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(16):\n",
    "    ax = fig.add_subplot(4, 4, i + 1)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.imshow(mnist.train.images[i].reshape(28, 28), cmap='Greys_r')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Training the model\n",
    "\n",
    "You should now train your neural network using minibatches of size 50. Try about 1000-5000 iterations. Keep track of the validation accuracy every 100 iterations, however. Once you are sure your optimisation is working properly, you should run the resulting model on the test data and report the test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a tf session and run the optimisation algorithm\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "num_epochs = 3000\n",
    "batch_size = 50\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    # Get a batch of training examples.\n",
    "    batch = mnist.train.next_batch(batch_size)\n",
    "    x_batch, y_batch= batch\n",
    "    \n",
    "    # Put the batch into a dict with the proper names\n",
    "    # for placeholder variables in the TensorFlow graph.\n",
    "    feed_dict_train = {x: x_batch, y_: y_batch}\n",
    "\n",
    "    # Run the optimizer using this batch of training data.\n",
    "    sess.run(train_step, feed_dict=feed_dict_train)\n",
    "    \n",
    "    # Print status every 100 iterations.\n",
    "    if i % 100 == 0:\n",
    "        \n",
    "        # Calculate the accuracy on the training-set.\n",
    "        acc = sess.run(accuracy, feed_dict=feed_dict_train)\n",
    "\n",
    "        # Message for printing.\n",
    "        msg = \"Optimization Iteration: {0:>6}, Training Accuracy: {1:>6.1%}\"\n",
    "\n",
    "        # Print it.\n",
    "        print(msg.format(i + 1, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print accuracy on the test set\n",
    " print ('Test accuracy: %g' % sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualising the Filters\n",
    "\n",
    "We'll now visualise all the 25 filters in the first convolution layer. As they are each of shape 12x12x1, they may themselves be viewed as greyscale images. Visualising filters in further layers is more complicated and involves modifying the neural network. See the [paper](http://www.matthewzeiler.com/pubs/arxive2013/arxive2013.pdf) by Matt Zeiler and Rob Fergus if you are interested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the filters in the first convolutional layer\n",
    "\n",
    "# Set W as the weights of each filter in conv1 layer, with shape=(12, 12, 1, 25)\n",
    "with sess.as_default():\n",
    "    W = W_conv1.eval()\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Weights for filters of first conv layer as greyscale images:', fontsize=12)\n",
    "num_columns = 5\n",
    "num_rows = math.ceil(num_filters_conv1 / num_columns)\n",
    "for i_filter in range(num_filters_conv1):\n",
    "    ax = fig.add_subplot(num_rows, num_columns, i_filter + 1)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set(xlabel='Filter #'+str(i_filter))\n",
    "    # Show weights of ith filter in conv1 and use greyscale\n",
    "    ax.imshow(W[:,:,0,i_filter], cmap='Greys_r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying image patches that activate the filters\n",
    "\n",
    "For this part you'll find the 12 patches in the test-set that activate each of the first 5 filters that maximise the activation for that filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-23348f83268d>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-23348f83268d>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    shape = H[:::0].shape()\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def plotNImagesForFilter(H, images, n, i_filter):\n",
    "    num_columns = 4\n",
    "    num_rows = math.ceil(n / num_columns)\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle('Top '+str(n)+' patches of test set that maximise activation for filter #'+str(i_filter)+':', fontsize=12)\n",
    "    for i_n in range(0, n, 1):\n",
    "        ax = fig.add_subplot(num_rows, num_columns, i_n + 1)\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        ax.imshow(H[i_n,:,:,i_filter], cmap='Greys_r')\n",
    "\n",
    "def totalFilterActivationForPatch(H, i_patch, i_filter):\n",
    "    return sum(sum(H[i_patch, :, :, i_filter]))\n",
    "\n",
    "# Visualise patches in the test set that find the most result in \n",
    "# the highest activations for filters 0, ... 4\n",
    "H =  sess.run(h_conv1, feed_dict={x: mnist.test.images})\n",
    "shape = H[:,:,:,0].shape()\n",
    "print(shape)\n",
    "# Plot for each of the first 5 filters...\n",
    "for i_filter in range(0, 5, 1):\n",
    "    totalActivationPerPatch = []\n",
    "    # Iterate over all patches in test set and calculate their filter activation\n",
    "    for i_patch in range(H.shape[0]):\n",
    "        totalActivationPerPatch.append(sum(sum(H[i_patch,:,:,i])))\n",
    "    # Get indeces of the 12 largest values in totalActivationPerPatch\n",
    "    max_12_patches = np.array(totalActivationPerPatch)\n",
    "    max_12_patches.argsort()[-12:][::-1]\n",
    "    \n",
    "    plotNImagesForFilter(H, max_12_patches, 12, i_filter)\n",
    "    \n",
    "max_val = H[:::0].argsort(axis= None)[-1:]\n",
    "index = np.unravel_index(max_val, shape)\n",
    "print(index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
